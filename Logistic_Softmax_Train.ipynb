{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Softmax_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9rD9Eg319AX2EfI/Jt7zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmay-47/Deep_Learning_Udemy/blob/master/Logistic_Softmax_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xde8pbIxyFto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from process import get_data\n",
        "\n",
        "def y2indicator(y, K):\n",
        "    N = len(y)\n",
        "    ind = np.zeros((N, K))\n",
        "    for i in range(N):\n",
        "        ind[i, y[i]] = 1\n",
        "    return ind\n",
        "\n",
        "Xtrain, Ytrain, Xtest, Ytest = get_data()\n",
        "D = Xtrain.shape[1]\n",
        "K = len(set(Ytrain) | set(Ytest))\n",
        "\n",
        "# convert to indicator\n",
        "Ytrain_ind = y2indicator(Ytrain, K)\n",
        "Ytest_ind = y2indicator(Ytest, K)\n",
        "\n",
        "# randomly initialize weights\n",
        "W = np.random.randn(D, K)\n",
        "b = np.zeros(K)\n",
        "\n",
        "# make predictions\n",
        "def softmax(a):\n",
        "    expA = np.exp(a)\n",
        "    return expA / expA.sum(axis=1, keepdims=True)\n",
        "\n",
        "def forward(X, W, b):\n",
        "    return softmax(X.dot(W) + b)\n",
        "\n",
        "def predict(P_Y_given_X):\n",
        "    return np.argmax(P_Y_given_X, axis=1)\n",
        "\n",
        "# calculate the accuracy\n",
        "def classification_rate(Y, P):\n",
        "    return np.mean(Y == P)\n",
        "\n",
        "def cross_entropy(T, pY):\n",
        "    return -np.mean(T*np.log(pY))\n",
        "\n",
        "\n",
        "# train loop\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "learning_rate = 0.001\n",
        "for i in range(10000):\n",
        "    pYtrain = forward(Xtrain, W, b)\n",
        "    pYtest = forward(Xtest, W, b)\n",
        "\n",
        "    ctrain = cross_entropy(Ytrain_ind, pYtrain)\n",
        "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
        "    train_costs.append(ctrain)\n",
        "    test_costs.append(ctest)\n",
        "\n",
        "    # gradient descent\n",
        "    W -= learning_rate*Xtrain.T.dot(pYtrain - Ytrain_ind)\n",
        "    b -= learning_rate*(pYtrain - Ytrain_ind).sum(axis=0)\n",
        "    if i % 1000 == 0:\n",
        "        print(i, ctrain, ctest)\n",
        "\n",
        "print(\"Final train classification_rate:\", classification_rate(Ytrain, predict(pYtrain)))\n",
        "print(\"Final test classification_rate:\", classification_rate(Ytest, predict(pYtest)))\n",
        "\n",
        "legend1, = plt.plot(train_costs, label='train cost')\n",
        "legend2, = plt.plot(test_costs, label='test cost')\n",
        "plt.legend([legend1, legend2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}