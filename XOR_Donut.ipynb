{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR_Donut.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIc+Vg70XnEKF5ZxVPj/5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmay-47/Deep_Learning_Udemy/blob/master/XOR_Donut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGxE55ZO1Mu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for binary classification! no softmax here\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    # sigmoid\n",
        "    # Z = 1 / (1 + np.exp( -(X.dot(W1) + b1) ))\n",
        "\n",
        "    # tanh\n",
        "    # Z = np.tanh(X.dot(W1) + b1)\n",
        "\n",
        "    # relu\n",
        "    Z = X.dot(W1) + b1\n",
        "    Z = Z * (Z > 0)\n",
        "\n",
        "    activation = Z.dot(W2) + b2\n",
        "    Y = 1 / (1 + np.exp(-activation))\n",
        "    return Y, Z\n",
        "\n",
        "\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    Y, _ = forward(X, W1, b1, W2, b2)\n",
        "    return np.round(Y)\n",
        "\n",
        "\n",
        "def derivative_w2(Z, T, Y):\n",
        "    # Z is (N, M)\n",
        "    return (T - Y).dot(Z)\n",
        "\n",
        "def derivative_b2(T, Y):\n",
        "    return (T - Y).sum()\n",
        "\n",
        "\n",
        "def derivative_w1(X, Z, T, Y, W2):\n",
        "    # dZ = np.outer(T-Y, W2) * Z * (1 - Z) # this is for sigmoid activation\n",
        "    # dZ = np.outer(T-Y, W2) * (1 - Z * Z) # this is for tanh activation\n",
        "    dZ = np.outer(T-Y, W2) * (Z > 0) # this is for relu activation\n",
        "    return X.T.dot(dZ)\n",
        "\n",
        "\n",
        "def derivative_b1(Z, T, Y, W2):\n",
        "    # dZ = np.outer(T-Y, W2) * Z * (1 - Z) # this is for sigmoid activation\n",
        "    # dZ = np.outer(T-Y, W2) * (1 - Z * Z) # this is for tanh activation\n",
        "    dZ = np.outer(T-Y, W2) * (Z > 0) # this is for relu activation\n",
        "    return dZ.sum(axis=0)\n",
        "\n",
        "\n",
        "def get_log_likelihood(T, Y):\n",
        "    return np.sum(T*np.log(Y) + (1-T)*np.log(1-Y))\n",
        "\n",
        "\n",
        "\n",
        "def test_xor():\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    Y = np.array([0, 1, 1, 0])\n",
        "    W1 = np.random.randn(2, 5)\n",
        "    b1 = np.zeros(5)\n",
        "    W2 = np.random.randn(5)\n",
        "    b2 = 0\n",
        "    LL = [] # keep track of log-likelihoods\n",
        "    learning_rate = 1e-2\n",
        "    regularization = 0.\n",
        "    last_error_rate = None\n",
        "    for i in range(30000):\n",
        "        pY, Z = forward(X, W1, b1, W2, b2)\n",
        "        ll = get_log_likelihood(Y, pY)\n",
        "        prediction = predict(X, W1, b1, W2, b2)\n",
        "        er = np.mean(prediction != Y)\n",
        "\n",
        "        LL.append(ll)\n",
        "        W2 += learning_rate * (derivative_w2(Z, Y, pY) - regularization * W2)\n",
        "        b2 += learning_rate * (derivative_b2(Y, pY) - regularization * b2)\n",
        "        W1 += learning_rate * (derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
        "        b1 += learning_rate * (derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
        "        if i % 1000 == 0:\n",
        "            print(ll)\n",
        "\n",
        "    print(\"final classification rate:\", np.mean(prediction == Y))\n",
        "    plt.plot(LL)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_donut():\n",
        "    # donut example\n",
        "    N = 1000\n",
        "    R_inner = 5\n",
        "    R_outer = 10\n",
        "\n",
        "    # distance from origin is radius + random normal\n",
        "    # angle theta is uniformly distributed between (0, 2pi)\n",
        "    R1 = np.random.randn(N//2) + R_inner\n",
        "    theta = 2*np.pi*np.random.random(N//2)\n",
        "    X_inner = np.concatenate([[R1 * np.cos(theta)], [R1 * np.sin(theta)]]).T\n",
        "\n",
        "    R2 = np.random.randn(N//2) + R_outer\n",
        "    theta = 2*np.pi*np.random.random(N//2)\n",
        "    X_outer = np.concatenate([[R2 * np.cos(theta)], [R2 * np.sin(theta)]]).T\n",
        "\n",
        "    X = np.concatenate([ X_inner, X_outer ])\n",
        "    Y = np.array([0]*(N//2) + [1]*(N//2))\n",
        "\n",
        "    n_hidden = 8\n",
        "    W1 = np.random.randn(2, n_hidden)\n",
        "    b1 = np.random.randn(n_hidden)\n",
        "    W2 = np.random.randn(n_hidden)\n",
        "    b2 = np.random.randn(1)\n",
        "    LL = [] # keep track of log-likelihoods\n",
        "    learning_rate = 0.00005\n",
        "    regularization = 0.2\n",
        "    last_error_rate = None\n",
        "    for i in range(3000):\n",
        "        pY, Z = forward(X, W1, b1, W2, b2)\n",
        "        ll = get_log_likelihood(Y, pY)\n",
        "        prediction = predict(X, W1, b1, W2, b2)\n",
        "        er = np.abs(prediction - Y).mean()\n",
        "        LL.append(ll)\n",
        "        W2 += learning_rate * (derivative_w2(Z, Y, pY) - regularization * W2)\n",
        "        b2 += learning_rate * (derivative_b2(Y, pY) - regularization * b2)\n",
        "        W1 += learning_rate * (derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
        "        b1 += learning_rate * (derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
        "        if i % 300 == 0:\n",
        "            print(\"i:\", i, \"ll:\", ll, \"classification rate:\", 1 - er)\n",
        "    plt.plot(LL)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # test_xor()\n",
        "    test_donut()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}