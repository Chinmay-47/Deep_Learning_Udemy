{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoJKxlM02hTYkeWxHRZv4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmay-47/Deep_Learning_Udemy/blob/master/ANN_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYl557n8y0qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from process import get_data\n",
        "\n",
        "def y2indicator(y, K):\n",
        "    N = len(y)\n",
        "    ind = np.zeros((N, K))\n",
        "    for i in range(N):\n",
        "        ind[i, y[i]] = 1\n",
        "    return ind\n",
        "\n",
        "Xtrain, Ytrain, Xtest, Ytest = get_data()\n",
        "D = Xtrain.shape[1]\n",
        "K = len(set(Ytrain) | set(Ytest))\n",
        "M = 5 # num hidden units\n",
        "\n",
        "# convert to indicator\n",
        "Ytrain_ind = y2indicator(Ytrain, K)\n",
        "Ytest_ind = y2indicator(Ytest, K)\n",
        "\n",
        "# randomly initialize weights\n",
        "W1 = np.random.randn(D, M)\n",
        "b1 = np.zeros(M)\n",
        "W2 = np.random.randn(M, K)\n",
        "b2 = np.zeros(K)\n",
        "\n",
        "# make predictions\n",
        "def softmax(a):\n",
        "    expA = np.exp(a)\n",
        "    return expA / expA.sum(axis=1, keepdims=True)\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    Z = np.tanh(X.dot(W1) + b1)\n",
        "    return softmax(Z.dot(W2) + b2), Z\n",
        "\n",
        "def predict(P_Y_given_X):\n",
        "    return np.argmax(P_Y_given_X, axis=1)\n",
        "\n",
        "# calculate the accuracy\n",
        "def classification_rate(Y, P):\n",
        "    return np.mean(Y == P)\n",
        "\n",
        "def cross_entropy(T, pY):\n",
        "    return -np.mean(T*np.log(pY))\n",
        "\n",
        "\n",
        "# train loop\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "learning_rate = 0.001\n",
        "for i in range(10000):\n",
        "    pYtrain, Ztrain = forward(Xtrain, W1, b1, W2, b2)\n",
        "    pYtest, Ztest = forward(Xtest, W1, b1, W2, b2)\n",
        "\n",
        "    ctrain = cross_entropy(Ytrain_ind, pYtrain)\n",
        "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
        "    train_costs.append(ctrain)\n",
        "    test_costs.append(ctest)\n",
        "\n",
        "    # gradient descent\n",
        "    W2 -= learning_rate*Ztrain.T.dot(pYtrain - Ytrain_ind)\n",
        "    b2 -= learning_rate*(pYtrain - Ytrain_ind).sum(axis=0)\n",
        "    dZ = (pYtrain - Ytrain_ind).dot(W2.T) * (1 - Ztrain*Ztrain)\n",
        "    W1 -= learning_rate*Xtrain.T.dot(dZ)\n",
        "    b1 -= learning_rate*dZ.sum(axis=0)\n",
        "    if i % 1000 == 0:\n",
        "        print(i, ctrain, ctest)\n",
        "\n",
        "print(\"Final train classification_rate:\", classification_rate(Ytrain, predict(pYtrain)))\n",
        "print(\"Final test classification_rate:\", classification_rate(Ytest, predict(pYtest)))\n",
        "\n",
        "legend1, = plt.plot(train_costs, label='train cost')\n",
        "legend2, = plt.plot(test_costs, label='test cost')\n",
        "plt.legend([legend1, legend2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}